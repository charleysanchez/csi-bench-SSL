training:
  batch_size: 32
  epochs: 100
  warmup_epochs: 5
  patience: 20
model_params:
  win_len: 500
  feature_size: 232
optimizer:
  lr: 0.001
  weight_decay: 1.0e-05
  name: adamw
scheduler:
  name: warmup_cosine
general:
  data_dir: data
  task: null
  all_tasks: true
  model: timesformer1d
  save_dir: pretrain_results
  seed: 42
  num_workers: 8
  pin_memory: false
  config: configs/pretrain.yaml
  min_lr_ratio: 0.0
